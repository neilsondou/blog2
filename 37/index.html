<html>
  <head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>关于Taalas的LLM方案：我们离未来最近的一次 | 老窦的博客</title>
<link rel="shortcut icon" href="https://dou.asia/favicon.ico?v=1771681436905">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://dou.asia/styles/main.css">

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>

<!-- DEMO JS -->
<script src="media/scripts/index.js"></script>



  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              关于Taalas的LLM方案：我们离未来最近的一次
            </h2>
            <div class="post-info">
              <time class="post-time">
                · 2026-02-21 ·
              </time>
              
            </div>
            
            <div class="post-content">
              <p>在2月18号，知乎上就有人问：有没有可能将大语言模型的架构芯片化，权重固件化？一天以后，Taalas发布了他们的方案：<br>
<img src="https://dou.asia/post-images/1771678296727.png" alt="" loading="lazy"></p>
<p>经过我自己的测试，去除掉网络的响应时间，几乎是按下按钮立刻获得答案，因为其本身使用的基座模型参数很小，所以现在回复上的质量很低，但是在速度上可以说前所未有，是真的能达到16k/s的输出速度。<br>
这个技术是这么实现的：<em>“我们基本上采用了一种嵌入式架构，将模型和权重硬编码到我们称之为掩模ROM调用架构的结构中，该架构与SRAM调用架构配合使用。它们共同能够存储模型并执行KV缓存的所有计算。我们提供适配器和定制方案——我们支持所有这些。这种设计使我们能够在计算和存储方面实现超高密度，并且我们可以在该存储上进行极快的计算，这正是提高密度和降低成本的关键所在。”</em><br>
最近新出现的方案，groq、cerebras等等，不论是审美tpu、npu还是什么pu，还是拘泥于传统思路，降本增效，不论是从x86换成arm，还是增大核心规模，还是缩短带宽延迟等等，我认为都没有从硬件上下手来的实在，如果说这些厂商在做的是在超越摩尔定律，那我觉得taalas的方案就是在颠覆摩尔定律。<br>
在相关报道中，很有意思的是一句*”由于 HC1 卡速度极快，无需批量处理查询即可实现低延迟推理，这意味着 Taalas 设备的带宽压力很低。低到即使将多张卡并联运行更大的模型，PCI-Express 总线也完全够用。“*这是否在影射老黄和他的nvlink？新出现的厂商，说他们做的东西很牛，要和老黄掰手腕，那我们得先算一下，这个技术比老黄的方案要强到哪里。<br>
我们先从集群算力的成本上看，taalas hc1与传统方案的差异几乎难以放在一起比较。b200单卡功耗达1000瓦，一个标准八卡服务器节点满载就要吃掉10-12千瓦的电力，而hc1每颗芯片仅250瓦，用50颗拼出12.5千瓦的总功耗虽然与b200集群持平，但前者能在1秒内生成上万tokens，后者却要耗时近一分钟，在完成相同任务时，hc1的电费仅是b200的一个零头。<br>
制造上的成本，现在我们可知的是hc1采用6纳米工艺、815毫米方的一块大芯片，裸片成本约100-150美元，加上简单的封装和内存控制器，总物料成本不超过400美元，即便分摊一次性开模费（约 100 万美元）后，每片成本也仅增加几十美元。b200的单卡售价动辄数万美元，且必须搭配高价hbm和3d堆叠，搭建一套能运行Qwen 3.5级别的50卡hc1集群，总硬件成本可能控制在5万美元左右，而同等算力的传统算力集群至少要贵一个数量级。从成本上看，我认为如果仅针对单一模型进行进一步的优化，可能进一步的节约成本，这还是个起步项目，该技术继续发展下去，可能会让这个成本的区别渐渐变得更大。<br>
成本上完爆老黄的方案，那问题在哪？现在我觉得关于这个技术，我们只需要关注两点：第一就是这个公司会不会被其他大公司收购，如果有人收购这个初创公司了，说明他们的技术是真的，并且可验证的，如果没有人在意这个技术，那么有很大可能这是个插标卖首忽悠投资的项目，第二点就是硬件与权重本身的强绑定，换一套模型就要换一套硬件，这个问题如果大厂把这个方案买下来我觉得问题也不是很大，现在可见的就是模型性能发展的放缓，算力和人类使用过程中可感的模型性能逐渐的不成线性关系了，这是通用模型，一年前的模型放现在未必不能用，何况这个技术的发展还需要时间，到时候再过五年可能llm的极限已经被人探索出来了，垂直领域的模型，优化起来更容易，发展也更慢，现在去担忧这个方案中，硬件的更新可能比模型的更新慢，或许有点太高看人类的创新能力了。<br>
为什么我说这个技术是我们离未来最近的一次，那我们看看这个技术如果实际落地、大公司参与硬件设计、可以用很低的成本几乎可以忽略的响应延迟来输出的前提下，有可能是怎么样的：<br>
1、机器人技术的最优解，运动机器人的最大问题一直都是延迟，低功耗低负载场景下，机器人的算力需求比llm要低很多，运动模型的参数一般直到亿这个级别，针对性的做一套用于机器人的大参数模型，然后在相同的功耗和负载下完成更好的工作，对于这个方案肯定是小菜一碟。<br>
2、如果可以本地低成本运行的，面向个人的，输出极快的方案可以普及，放在pc里面啊，放在手机里面等等，会在某些想不到的应用场景带来质变，我举一个例子，就是现在流行的vibe coding，我自己也深受输出速度慢其害，如果在开发过程中有一个这种输出速度的模型，那么我有胆量一天vibe coding出任何人类见过的东西。<br>
3、gan、蒸馏的大成功，毫无疑问，如果模型速度以后都这么快了，gan起来有多么方便，甚至实现模型自己的快速进化也不是没有可能。<br>
还有很多大家都能想到的具体应用，如果某个厂商发布了新的芯片，说算力提高了多少多少倍，降低了多少多少成本，现在的人可能都对这种消息免疫了，不在被老黄画的大饼忽悠，以为你的卡再厉害和消费者也没关系，但是这套方案不仅是算力质的提升，同时也和我们消费者有关，以后买个谁家的算力卡，插到pcie上，就能享受16k/s的速度了，那简直太爽了，然后以后的机器人，几百瓦的功耗，不仅能翻山越岭，还内置一个llm能聊天，不用联网，和他聊天几乎没有延迟，更遑论工业领域了。</p>

            </div>
          </article>
        </div>
    
        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://dou.asia/36/">
              <h3 class="post-title">
                Bewy编辑器—你的下一款vscode，何必是vscode？
              </h3>
            </a>
          </div>  
        

        
    
        <div class="site-footer">
  CC BY 4.0
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

      </div>
    </div>
  </body>
</html>
